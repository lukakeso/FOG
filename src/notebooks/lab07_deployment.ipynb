{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZpuxz39H2mf"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45D6GuSwvT7d"
      },
      "outputs": [],
      "source": [
        "!pip install pip==24.0\n",
        "if \"bootstrap\" not in locals() or bootstrap.run:\n",
        "    # path management for Python\n",
        "    pythonpath, = !echo $PYTHONPATH\n",
        "    if \".\" not in pythonpath.split(\":\"):\n",
        "        pythonpath = \".:\" + pythonpath\n",
        "        %env PYTHONPATH={pythonpath}\n",
        "        !echo $PYTHONPATH\n",
        "\n",
        "    # get both Colab and local notebooks into the same state\n",
        "    !wget --quiet https://raw.githubusercontent.com/lukakeso/FOG/main/bootstrap.py -O bootstrap.py\n",
        "    import bootstrap\n",
        "\n",
        "    # change into the correct directory\n",
        "    FOG = bootstrap.get_repo()\n",
        "    bootstrap.change_to_lab_dir()\n",
        "\n",
        "    bootstrap.run = False  # change to True re-run setup\n",
        "\n",
        "!pwd\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzi8qYKI-njP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from IPython.display import display, HTML, IFrame\n",
        "\n",
        "full_width = True\n",
        "frame_height = 720  # adjust for your screen\n",
        "\n",
        "if full_width:  # if we want the notebook to take up the whole width\n",
        "    # add styling to the notebook's HTML directly\n",
        "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93pc-NLrBR1A"
      },
      "source": [
        "To recap, our model staging workflow,\n",
        "which does the hand-off between training and production, looks like this:\n",
        "\n",
        "1. Get model weights and hyperparameters\n",
        "from a tracked training run in W&B's cloud storage.\n",
        "2. Reload the model as a `LightningModule` using those weights and hyperparameters.\n",
        "3. Call `to_torchscript` on it.\n",
        "4. Save that result to W&B's cloud storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4qEqMRkFsd4"
      },
      "source": [
        "Here in this notebook,\n",
        "rather than training or scripting a model ourselves,\n",
        "we'll just `--fetch`\n",
        "an already trained and scripted model binary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2wfjLmRDwrH"
      },
      "outputs": [],
      "source": [
        "%run training/stage_model.py --fetch --entity=cfrye59 --from_project=FOG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-MSRHrjH2mm"
      },
      "source": [
        "## Running our more portable model via a CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7d2WHSCHHHP"
      },
      "source": [
        "Now that our TorchScript model binary file is present,\n",
        "we can spin up our text recognizer\n",
        "with much less code.\n",
        "\n",
        "We just need a compatible version of PyTorch\n",
        "and methods to convert\n",
        "our generic data types\n",
        "(images, strings)\n",
        "to and from PyTorch `Tensor`s.\n",
        "\n",
        "We can put all this together in\n",
        "a single light-weight object,\n",
        "the `ParagraphTextRecognizer` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGXZep-nDiDk"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.paragraph_text_recognizer import ParagraphTextRecognizer\n",
        "\n",
        "ptr = ParagraphTextRecognizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwVo6BoeGmTW"
      },
      "source": [
        "And from there,\n",
        "we can start running on images\n",
        "and inferring the text that they contain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZlfIoeG3hy"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "example_input = \"text_recognizer/tests/support/paragraphs/a01-077.png\"\n",
        "\n",
        "print(ptr.predict(example_input))\n",
        "Image(example_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te-CYidTslPo"
      },
      "source": [
        "# Building a simple model UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xey5gzr5tV51"
      },
      "source": [
        "We use the\n",
        "[`gradio` library](https://gradio.app/),\n",
        "which includes a simple API for wrapping\n",
        "a single Python function into a frontend\n",
        "in addition to a less mature, lower-level API\n",
        "for building apps more flexibly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XvUr7irMHQ6"
      },
      "source": [
        "The core component is a script,\n",
        "`app_gradio/app.py`,\n",
        "that can be used to spin up our model and UI\n",
        "from the command line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2Ra8ot292XX"
      },
      "outputs": [],
      "source": [
        "%run app_gradio/app.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9bP3zFo9_YY"
      },
      "source": [
        "One very nice feature of `gradio`\n",
        "is that it is designed to run as easily\n",
        "from the notebook as from the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vryi5r6gDj6D"
      },
      "outputs": [],
      "source": [
        "from app_gradio import app\n",
        "\n",
        "frontend = app.make_frontend(ptr.predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0HxOukBNn13"
      },
      "source": [
        "\n",
        "\n",
        "We can spin up our UI with the `.launch` method,\n",
        "and now we can interact\n",
        "with the model from inside the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoVFtGbuDlTL"
      },
      "outputs": [],
      "source": [
        "frontend.launch(share=True, width=\"100%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okcoAW7sM13h"
      },
      "source": [
        "For 72 hours, we can also access the model over the public internet\n",
        "using a URL provided by `gradio`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5pEhMECNIT6"
      },
      "outputs": [],
      "source": [
        "print(frontend.share_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYfi-lZqNNZd"
      },
      "source": [
        "You can point your browser to that URL\n",
        "to see what the model looks like as a full-fledged web application,\n",
        "instead of a widget inside the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd1UZiM3ZVWz"
      },
      "source": [
        "Once done,\n",
        "turn off the Gradio interface by running the `.close` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVyv6KjxJhEb"
      },
      "outputs": [],
      "source": [
        "frontend.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BpPtj6tsP-Y"
      },
      "source": [
        "# Wrapping a model into a model service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ButF0a6PSbMi"
      },
      "source": [
        "With the current setup our model is running in the same place as our frontend.\n",
        "\n",
        "This is simple, but it ties too many things together.\n",
        "\n",
        "First, it ties together execution of the two components.\n",
        "\n",
        "If our ML model stops responding or there is a DNN bug,\n",
        "the server goes down.\n",
        "The same applies in reverse --\n",
        "the only API for the model is provided by `gradio`,\n",
        "so a frontend issue means the model is inaccessible.\n",
        "\n",
        "That's bad because the server and the model scale differently.\n",
        "Running the server at scale has different memory and computational requirements\n",
        "than does running the model at scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WI0H6Imcz_h"
      },
      "source": [
        "Luckily, there is an easier way: \"serverless cloud functions\",\n",
        "so named because\n",
        "- they are run intermittently, rather than 24/7, like a server.\n",
        "- they are run on cloud infrastructure.\n",
        "- they are, as in\n",
        "[purely functional programming](https://en.wikipedia.org/wiki/Purely_functional_programming)\n",
        "or in mathematics, \"pure\" functions of their inputs,\n",
        "with no concept of state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE_FhWxLhhxG"
      },
      "source": [
        "We use AWS's serverless offering,\n",
        "[AWS Lambda](https://aws.amazon.com/lambda/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw3Una-yJ_mP"
      },
      "outputs": [],
      "source": [
        "from api_serverless import api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76HwEP2Vzz3F"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from IPython.display import Image\n",
        "import requests  # the preferred library for writing HTTP requests in Python\n",
        "\n",
        "lambda_url = \"https://3akxma777p53w57mmdika3sflu0fvazm.lambda-url.us-west-1.on.aws/\"\n",
        "image_url = \"https://fsdl-public-assets.s3-us-west-2.amazonaws.com/paragraphs/a01-077.png\"\n",
        "\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "payload = json.dumps({\"image_url\": image_url})\n",
        "\n",
        "response = requests.post(  # we POST the image to the URL, expecting a prediction as a response\n",
        "    lambda_url, data=payload, headers=headers)\n",
        "pred = response.json()[\"pred\"]  # the response is also json\n",
        "\n",
        "print(pred)\n",
        "\n",
        "Image(url=image_url, width=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQ2Dt4URN9o"
      },
      "source": [
        "## Local in the front, serverless in the back"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMXWTHt4Pxpr"
      },
      "source": [
        "The primary \"win\" here\n",
        "is that we don't need to run\n",
        "the frontend UI server\n",
        "and the backend model service in\n",
        "the same place.\n",
        "\n",
        "For example,\n",
        "we can run a Gradio app locally\n",
        "but send the images to the serverless function\n",
        "for prediction.\n",
        "\n",
        "Our `app_gradio` implementation supports this via the `PredictorBackend`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qZ1K0fwOtYK"
      },
      "outputs": [],
      "source": [
        "serverless_backend = app.PredictorBackend(url=lambda_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKA68zxUUO9e"
      },
      "source": [
        "The frontend doesn't care where the inference is getting done or how.\n",
        "\n",
        "A `gradio.Interface`\n",
        "just knows there's a Python function that it invokes and then\n",
        "waits for outputs from.\n",
        "\n",
        "Here, that Python function\n",
        "makes a request to the serverless backend,\n",
        "rather than running the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEkMzohnOcK0"
      },
      "outputs": [],
      "source": [
        "frontend_serverless_backend = app.make_frontend(serverless_backend.run)\n",
        "\n",
        "frontend_serverless_backend.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}